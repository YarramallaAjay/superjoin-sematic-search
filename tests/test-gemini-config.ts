import { GoogleGenerativeAI } from "@google/generative-ai";
import { llmConfig, validateLLMConfig } from "../apps/config/llm-config";

async function testGeminiConfig() {
  console.log("üß™ Testing Gemini LLM Configuration...");
  
  // Validate configuration
  const isValid = validateLLMConfig();
  console.log(`Configuration valid: ${isValid}`);
  
  // Test Gemini initialization
  try {
    const geminiConfig = llmConfig.gemini;
    console.log("üìä Gemini Config:", {
      model: geminiConfig.model,
      temperature: geminiConfig.temperature,
      maxTokens: geminiConfig.maxTokens,
      hasApiKey: !!geminiConfig.apiKey
    });
    
    const genAI = new GoogleGenerativeAI(geminiConfig.apiKey);
    const model = genAI.getGenerativeModel({ 
      model: geminiConfig.model,
      generationConfig: {
        temperature: geminiConfig.temperature,
        maxOutputTokens: geminiConfig.maxTokens
      }
    });
    
    console.log("‚úÖ Gemini model created successfully");
    
    // Test a simple generation
    const prompt = "Hello! Please respond with 'Gemini is working!' and nothing else.";
    console.log("ü§ñ Testing generation with prompt:", prompt);
    
    const result = await model.generateContent(prompt);
    const response = result.response.text();
    
    console.log("‚úÖ Generation successful!");
    console.log("üìù Response:", response);
    
    if (response.includes("Gemini is working")) {
      console.log("üéâ Gemini LLM is fully functional!");
    } else {
      console.log("‚ö†Ô∏è Response format unexpected, but generation worked");
    }
    
  } catch (error) {
    console.error("‚ùå Gemini test failed:", error);
    if (error instanceof Error) {
      console.error("Error message:", error.message);
      console.error("Error stack:", error.stack);
    }
  }
}

// Run the test
testGeminiConfig().catch(console.error);
